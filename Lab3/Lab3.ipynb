{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#sve transponirano drugacije, mozda greska u backpropu kod self.W azuriranja.. i suma biasa... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5,  8],\n",
       "        [11, 18]],\n",
       "\n",
       "       [[11, 18],\n",
       "        [11, 18]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([[[1,2],[3,4]],[[3,4],[3,4]]]),np.array([[1,2],[2,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5  0.3  0.3]\n",
      "[2 1 0]\n",
      "[ 0.69314718  1.2039728   1.2039728 ] 1.0336975964\n",
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "minibatch_size = 3\n",
    "softmax_o = np.array([[0.2,0.3,0.5],[0.5,0.3,0.2],[0.3,0.2,0.5]])\n",
    "byz = np.array([[0,1,2],[1,1,1],[0,0,0]])\n",
    "loss2 = -np.log(softmax_o[range(minibatch_size),byz[range(minibatch_size),2]])\n",
    "print softmax_o[range(minibatch_size),byz[range(minibatch_size),2]]\n",
    "print byz[range(minibatch_size),2]\n",
    "loss2s = np.sum(loss2)/minibatch_size\n",
    "print loss2, loss2s\n",
    "print -np.log(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset modul valjda..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]]\n",
      "[[2 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [1 1]]\n",
      "[[3 1]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [2 2]]\n",
      "[2 2 2 2]\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "[[0 1 2]\n",
      " [1 0 3]\n",
      " [3 1 1]]\n",
      "\n",
      "[1 0 1]\n",
      "(3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "zip(['a','b','c'], range(3))\n",
    "temp_b = np.random.randint(2,size=(1,2))\n",
    "temp = np.random.randint(3,size=(4,2))\n",
    "print temp_b\n",
    "print temp\n",
    "print temp + temp_b\n",
    "print np.sum(temp,1)\n",
    "for i in reversed(range(5)):\n",
    "    print i\n",
    "\n",
    "test = np.random.randint(4,size=(3,3))\n",
    "print test\n",
    "print\n",
    "print test[:,1]\n",
    "\n",
    "t1 = np.random.randint(4,size=(2,2))\n",
    "t2 = np.random.randint(4,size=(2,2))\n",
    "t3 = np.random.randint(4,size=(2,2))\n",
    "tem = []\n",
    "tem.append(t1)\n",
    "tem.append(t2)\n",
    "tem.append(t3)\n",
    "print np.array(tem).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u't', u'e', u'\\n', u'a', u'c', u'b', u'f', u'i', u'l', u'3', u'1', u's', u'2', u'4']\n",
      "\n",
      "{u'a': 3, u'c': 4, u'b': 5, u'e': 1, u'f': 6, u'i': 7, u's': 11, u'\\n': 2, u'l': 8, u'1': 10, u'3': 9, u'2': 12, u't': 0, u'4': 13}\n",
      "\n",
      "{0: u't', 1: u'e', 2: u'\\n', 3: u'a', 4: u'c', 5: u'b', 6: u'f', 7: u'i', 8: u'l', 9: u'3', 10: u'1', 11: u's', 12: u'2', 13: u'4'}\n",
      "\n",
      "[ 0  1 11  0  6  7  8  1  2 10 12  9 13  2  3  5  4  0  0  0]\n",
      "\n",
      "[ 0  1 11  0]\n",
      "\n",
      "[u't' u't' u'e' u'e']\n",
      "\n",
      "minibatch\n",
      "None\n",
      "\n",
      "(False, array([[ 0,  1, 11,  0],\n",
      "       [ 2, 10, 12,  9]]), array([[ 1, 11,  0,  6],\n",
      "       [10, 12,  9, 13]]))\n",
      "\n",
      "(False, array([[ 6,  7,  8,  1],\n",
      "       [13,  2,  3,  5]]), array([[7, 8, 1, 2],\n",
      "       [2, 3, 5, 4]]))\n",
      "\n",
      "(True, array([[ 0,  1, 11,  0],\n",
      "       [ 2, 10, 12,  9]]), array([[ 1, 11,  0,  6],\n",
      "       [10, 12,  9, 13]]))\n",
      "\n",
      "(False, array([[ 6,  7,  8,  1],\n",
      "       [13,  2,  3,  5]]), array([[7, 8, 1, 2],\n",
      "       [2, 3, 5, 4]]))\n",
      "\n",
      "test (True, array([[ 0,  1, 11,  0],\n",
      "       [ 2, 10, 12,  9]]), array([[ 1, 11,  0,  6],\n",
      "       [10, 12,  9, 13]]))\n",
      "True\n",
      "[11 12]\n",
      "[[ 1 11  0  6]\n",
      " [10 12  9 13]]\n"
     ]
    }
   ],
   "source": [
    "# data = \"testfile\\n1234\\nabcttt\"\n",
    "# dictionary ={}\n",
    "# for letter in data:\n",
    "#     if letter in dictionary:\n",
    "#         dictionary[letter] += 1\n",
    "#     else:\n",
    "#         dictionary[letter] = 1\n",
    "# sorted_x = sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True)\n",
    "# print sorted_x\n",
    "# print map(lambda x: x[0],sorted_x)\n",
    "\n",
    "D = Dataset(2,4)\n",
    "D.preprocess(\"untitled.txt\")\n",
    "print D.sorted_chars\n",
    "print\n",
    "print D.char2id\n",
    "print\n",
    "print D.id2char\n",
    "print\n",
    "print D.x\n",
    "print\n",
    "print D.encode('test')\n",
    "print\n",
    "print D.decode([0,0,1,1])\n",
    "print\n",
    "print \"minibatch\"\n",
    "print D.create_minibatches()\n",
    "print\n",
    "print D.next_minibatch()\n",
    "print\n",
    "print D.next_minibatch()\n",
    "print\n",
    "print D.next_minibatch()\n",
    "print\n",
    "print D.next_minibatch()\n",
    "print\n",
    "test2 = D.next_minibatch()\n",
    "print \"test\", test2\n",
    "print test2[0]\n",
    "print test2[1][:,2]\n",
    "print test2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 12 12  8]\n",
      " [ 3  3  3  3]]\n",
      "[[10  3]\n",
      " [12  3]\n",
      " [12  3]\n",
      " [ 8  3]]\n",
      "[10 13]\n"
     ]
    }
   ],
   "source": [
    "dytest = np.random.randint(4,size=(3,2)) #minibatch x voc\n",
    "hiddentest = np.random.randint(4,size=(3,4)) #minibatch x hidden\n",
    "print np.dot(dytest.transpose(),hiddentest)\n",
    "print np.dot(hiddentest.transpose(),dytest)\n",
    "print np.dot(np.array([1,2]),np.array([[2,3],[4,5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "    # \"np\" stands for numpy.\n",
    "    def __init__(self, batch_size, sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_pointer = 0\n",
    "        \n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read().decode(\"utf-8\")\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        \n",
    "        dictionary ={}\n",
    "        for letter in data:\n",
    "            if letter in dictionary:\n",
    "                dictionary[letter] += 1\n",
    "            else:\n",
    "                dictionary[letter] = 1\n",
    "        sorted_x = sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        self.sorted_chars = map(lambda x: x[0],sorted_x)\n",
    "        \n",
    "        # self.sorted chars contains just the characters ordered descending by frequency\n",
    "        self.char2id = dict(zip(self.sorted_chars, range(len(self.sorted_chars)))) \n",
    "        # reverse the mapping\n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        # convert the data to ids\n",
    "        self.x = np.array(map(self.char2id.get, data))\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        # returns the sequence encoded as integers\n",
    "        return np.array(map(self.char2id.get, sequence))\n",
    "\n",
    "    def decode(self, encoded_sequence):\n",
    "        # returns the sequence decoded as letters\n",
    "        return np.array(map(self.id2char.get, encoded_sequence))\n",
    "\n",
    "    # ...\n",
    "    \n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "\n",
    "    def create_minibatches(self):\n",
    "        self.num_batches = int(len(self.x) / (self.batch_size * self.sequence_length)) # calculate the number of batches\n",
    "        # Is all the data going to be present in the batches? Why?\n",
    "        # What happens if we select a batch size and sequence length larger than the length of the data?\n",
    "\n",
    "        #######################################\n",
    "        #       Convert data to batches       #\n",
    "        #######################################\n",
    "        if len(self.x) < self.sequence_length+1:\n",
    "            raise Exception(\"Nedovoljno podataka za jednu kombinaciju ulaza/izlaza sa zadanim sequence lengthom\")\n",
    "            \n",
    "        if self.num_batches == 0:\n",
    "            raise Exception(\"Nemoguce napraviti niti jedan batch sa zadanim parametrima\")\n",
    "            \n",
    "        temp_inp_out_combinations = []    \n",
    "        for i in range(self.num_batches*self.batch_size): #kreiram sve kombinacije ulaz-izlaz pa cu ih kasnije spajat\n",
    "            inputs = np.array([elem for elem in self.x[(i)*self.sequence_length:(i+1)*self.sequence_length]])\n",
    "            targets = np.array([elem for elem in self.x[(i)*self.sequence_length+1:(i+1)*self.sequence_length+1]])\n",
    "            temp_inp_out_combinations.append((inputs,targets))\n",
    "            \n",
    "        self.batches = []\n",
    "        num_ins = len(temp_inp_out_combinations)\n",
    "        for i in range(num_ins):\n",
    "            inp, out = temp_inp_out_combinations[i]\n",
    "            if i < self.num_batches:\n",
    "                batch_x = []\n",
    "                batch_y = []\n",
    "                self.batches.append([])\n",
    "            else:\n",
    "                batch_x, batch_y = self.batches[i%self.num_batches]\n",
    "            batch_x.append(inp)\n",
    "            batch_y.append(out)\n",
    "            self.batches[i%self.num_batches] = (batch_x,batch_y)\n",
    "            \n",
    "        for i in range(self.num_batches):\n",
    "                batch_x, batch_y = self.batches[i]\n",
    "                self.batches[i] = (np.array(batch_x),np.array(batch_y))\n",
    "#         cnt = 0\n",
    "        \n",
    "#         for j in range(self.num_batches):\n",
    "#             batch = []\n",
    "#             batch_x = []\n",
    "#             batch_y = []\n",
    "#             for i in range(self.batch_size):\n",
    "#                 inputs = np.array([elem for elem in self.x[(cnt)*self.sequence_length:(cnt+1)*self.sequence_length]])\n",
    "#                 targets = np.array([elem for elem in self.x[(cnt)*self.sequence_length+1:(cnt+1)*self.sequence_length+1]])\n",
    "#                 batch_x.append(inputs)\n",
    "#                 batch_y.append(targets)\n",
    "#                 batch.append((inputs,targets))\n",
    "#                 cnt +=1\n",
    "#             self.batches.append((np.array(batch_x), np.array(batch_y)))\n",
    "            \n",
    "    \n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "    def next_minibatch(self):\n",
    "        # ...\n",
    "        new_epoch = False\n",
    "        if self.batch_pointer >= self.num_batches:\n",
    "            self.batch_pointer = 0\n",
    "            new_epoch = True\n",
    "            \n",
    "        batch_x, batch_y = self.batches[self.batch_pointer]\n",
    "        self.batch_pointer += 1\n",
    "        # handling batch pointer & reset\n",
    "        # new_epoch is a boolean indicating if the batch pointer was reset\n",
    "        # in this function call\n",
    "        return new_epoch, batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN dio..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "    # \"np\" stands for numpy\n",
    "    def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.U = np.random.normal(0, 1e-2,(vocab_size,hidden_size))# ... input projection\n",
    "        self.W = np.random.normal(0, 1e-2,(hidden_size, hidden_size)) # ... hidden-to-hidden projection\n",
    "        self.b = np.random.normal(0, 1e-2,(1, hidden_size)) # ... input bias\n",
    "        \n",
    "        self.V = np.random.normal(0, 1e-2,(hidden_size,vocab_size)) # ... output projection\n",
    "        self.c = np.random.normal(0, 1e-2,(1,vocab_size)) # ... output bias\n",
    "\n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        \n",
    "        \n",
    "    def encode_1_of_k_rep(self, encoded_sequence): #encodes array of integers for a batch to 1-of-k representation\n",
    "        minibatches = encoded_sequence.shape[0]\n",
    "        new_sequence = np.zeros((minibatches, self.vocab_size)) # encode in 1-of-k representation\n",
    "        new_sequence[range(minibatches),encoded_sequence.astype(np.int32)] = 1\n",
    "        return new_sequence.astype(np.int32)\n",
    "        \n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "    def rnn_step_forward(self, x, h_prev):\n",
    "        #x - minibatch x vocab\n",
    "        #h_prev - minibatch x hidden size\n",
    "\n",
    "        cache = {}\n",
    "        hidden_state = np.tanh(np.dot(x, self.U) + np.dot(h_prev,self.W) + self.b) #minibatch x hidden\n",
    "        output = np.dot(hidden_state, self.V) + self.c #minibatch x vocab_size\n",
    "        soft_sum = np.sum(np.exp(output),1)\n",
    "        softmax_output = np.exp(output)/(soft_sum[:,None]) #minibatch x vocab_size\n",
    "        \n",
    "        cache[\"input\"] = x\n",
    "        cache[\"hprev\"] = h_prev\n",
    "        cache[\"hstate\"] = hidden_state\n",
    "        cache[\"output\"] = output\n",
    "        cache[\"softmax_out\"] = softmax_output\n",
    "        \n",
    "        return hidden_state.copy(), cache\n",
    "\n",
    "\n",
    "    def rnn_forward(self, x, h0):\n",
    "        \n",
    "        #x je meni shape minibatch size x sequence_length x 1 gdje je 1 zapravo integer reprezentacija charactera\n",
    "        #koju encodiram prije predaje funkciji\n",
    "        h, cache = {}, {}\n",
    "        h[-1] = h0\n",
    "        hprev = h0\n",
    "        minibatches, seq_length = x.shape\n",
    "        for i in range(self.sequence_length):\n",
    "            current_timestep_inputs = x[range(minibatches),i]\n",
    "            current_timestep_inputs_encoded = self.encode_1_of_k_rep(current_timestep_inputs) # minibatch x vocab_size\n",
    "            new_hidden, new_cache = self.rnn_step_forward(current_timestep_inputs_encoded, hprev)\n",
    "            hprev = new_hidden\n",
    "            cache[i] = new_cache\n",
    "            h[i]= new_hidden.copy()\n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "\n",
    "        return h, cache\n",
    "    \n",
    "    def rnn_BPTT(self, batch_x, batch_y, cache_dict, h0):\n",
    "        #konvencija imenovanja derivacija je W tezine a mali znakovi # -> # odnosno matrica koja preslikava hidden u hidden je hh\n",
    "        # input u hidden je xh , hidden u output hy, a ovo self.U ostaje jer je takav template iz zadatka :S sredim kasnije... valjda\n",
    "        \n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        dbh, dby = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dhnext = np.zeros_like(h0) #minibatch x hidden_size\n",
    "        minibatch_size = h0.shape[0]\n",
    "        loss = 0\n",
    "        cache_length = len(cache_dict.keys())\n",
    "        for i in reversed(range(cache_length)):\n",
    "            ## compute derivative of error w.r.t the output probabilites\n",
    "            ## dE/dy[j] = y[j] - t[j]\n",
    "            \n",
    "            x_input = cache_dict[i][\"input\"]\n",
    "            h_prev = cache_dict[i][\"hprev\"] #minibatch x hidden\n",
    "            hidden_state = cache_dict[i][\"hstate\"] #minibatch x hidden_size\n",
    "            output = cache_dict[i][\"output\"]\n",
    "            softmax_output =cache_dict[i][\"softmax_out\"] #minibatch x vocab\n",
    "            \n",
    "            dy = np.copy(softmax_output)\n",
    "            dy[range(minibatch_size),batch_y[range(minibatch_size),i]] -= 1 # backprop into y, size = minibatch x vocab_size\n",
    "\n",
    "            ## output layer doesnot use activation function, so no need to compute the derivative of error with regard to the net input\n",
    "            ## of output layer. \n",
    "            ## then, we could directly compute the derivative of error with regard to the weight between hidden layer and output layer.\n",
    "            ## dE/dy[j]*dy[j]/dWhy[j,k] = dE/dy[j] * h[k]\n",
    "\n",
    "            dWhy += np.dot(hidden_state.T, dy)\n",
    "            dby += np.sum(dy,0)\n",
    "            \n",
    "            ## backprop into h\n",
    "            ## derivative of error with regard to the output of hidden layer\n",
    "            ## derivative of H, come from output layer y and also come from H(t+1), the next time H\n",
    "            dh = np.dot(dy, self.V.T) + dhnext\n",
    "            ## backprop through tanh nonlinearity\n",
    "            ## derivative of error with regard to the input of hidden layer\n",
    "            ## dtanh(x)/dx = 1 - tanh(x) * tanh(x)\n",
    "            dhraw = (1 - hidden_state*hidden_state) * dh #minibatch x hidden\n",
    "            dbh += np.sum(dhraw,0)\n",
    "\n",
    "            ## derivative of the error with regard to the weight between input layer and hidden layer\n",
    "            dWxh += np.dot(x_input.T,dhraw) # vocab x hidden\n",
    "            dWhh += np.dot(h_prev.T, dhraw) # hidden x hidden\n",
    "            ## derivative of the error with regard to H(t+1)\n",
    "            ## or derivative of the error of H(t-1) with regard to H(t)\n",
    "            dhnext = np.dot(dhraw, self.W) #minibatch x hidden\n",
    "            \n",
    "            loss += -np.log(softmax_output[range(minibatch_size),batch_y[range(minibatch_size),i]])\n",
    "        \n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam/minibatch_size, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "            \n",
    "        loss = np.sum(loss)/minibatch_size\n",
    "        return loss, dWxh, dWhh, dWhy, dbh, dby, cache_dict[cache_length-1][\"hstate\"]\n",
    "    \n",
    "    def step(self, h0, batch_x, batch_y):\n",
    "        hidden_dict, cache_dict = self.rnn_forward(batch_x,h0)\n",
    "        loss, dU, dW, dV, db, dc, new_h0 = self.rnn_BPTT(batch_x, batch_y, cache_dict, h0)\n",
    "        self.update(dU, dW, db, dV, dc)\n",
    "        return loss, new_h0\n",
    "    \n",
    "    \n",
    "    def update(self, dU, dW, db, dV, dc):\n",
    "\n",
    "        # update memory matrices\n",
    "        # perform the Adagrad update of parameters\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.b, self.V, self.c],\n",
    "                                [dU, dW, db, dV, dc],\n",
    "                                [self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c]):\n",
    "            mem += dparam * dparam\n",
    "            ## learning_rate is adjusted by mem, if mem is getting bigger, then learning_rate will be small\n",
    "            ## gradient descent of Adagrad\n",
    "            param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "    \n",
    "    # ...\n",
    "    # code not necessarily nested in class definition\n",
    "    def sample(self, dataset, seed = 'HAN:\\nIs that good or bad?\\n\\n', n_sample=300):\n",
    "        hidden = np.zeros((1, self.hidden_size))\n",
    "        encoded = dataset.encode(seed)\n",
    "        one_hot = self.encode_1_of_k_rep(encoded)\n",
    "        # inicijalizirati h0 na vektor nula\n",
    "        # seed string pretvoriti u one-hot reprezentaciju ulaza\n",
    "        output_string = \"\"\n",
    "        last_output = \"\"\n",
    "        seed_length = len(seed)\n",
    "        for inp in one_hot:\n",
    "            hidden, cache = self.rnn_step_forward(inp, hidden)\n",
    "            output = np.random.choice(range(self.vocab_size), p=cache[\"softmax_out\"].ravel())\n",
    "            last_output = np.array([output])\n",
    "            #output_string+=dataset.decode(last_output)[0]\n",
    "            \n",
    "        for k in range(n_sample-seed_length):\n",
    "            last_output_encoded = self.encode_1_of_k_rep(last_output)\n",
    "            hidden, cache = self.rnn_step_forward(last_output_encoded, hidden)\n",
    "            output = np.random.choice(range(self.vocab_size), p=cache[\"softmax_out\"].ravel())\n",
    "            last_output = np.array([output])\n",
    "            output_string+=dataset.decode(last_output)[0]\n",
    "            \n",
    "        return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.4454671309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a625f9257284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minit_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwanted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ebc4d1b84cb6>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, h0, batch_x, batch_y)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mhidden_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_BPTT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ebc4d1b84cb6>\u001b[0m in \u001b[0;36mrnn_BPTT\u001b[0;34m(self, batch_x, batch_y, cache_dict, h0)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m## dtanh(x)/dx = 1 - tanh(x) * tanh(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mdhraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m \u001b[0;31m#minibatch x hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mdbh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m## derivative of the error with regard to the weight between input layer and hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1828\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1830\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1831\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testRNN = RNN(10,50,5,0.01)\n",
    "# h, cache = testRNN.rnn_forward(np.array([[1.,2.,1.]]),np.zeros((1,10)))\n",
    "# for key in cache.keys():\n",
    "#     #print key, cache[key][\"output\"], cache[key][\"input\"]\n",
    "#     pass\n",
    "\n",
    "inp = np.array([[1.,2.,1.,4.,3.]*10,[3.,1.,3.,2.,4.]*10])\n",
    "wanted = np.array([[2,2,1,3,3]*10,[1,2,3,1,1]*10])\n",
    "init_state = np.zeros((2,10))\n",
    "for i in range(5000):\n",
    "    loss, new_h0 = testRNN.step(init_state.astype(np.float32),inp,wanted)\n",
    "    if i % 1000 == 0:\n",
    "        print loss\n",
    "\n",
    "        \n",
    "h, cache = testRNN.rnn_forward(inp,init_state.astype(np.float32))\n",
    "tocni, netocni = 0,0\n",
    "for key in cache.keys():\n",
    "    netocni_current = np.count_nonzero(np.argmax(cache[key][\"output\"],1)-wanted[:,key])\n",
    "    tocni +=2-netocni_current\n",
    "    netocni+=netocni_current\n",
    "#     print key, np.argmax(cache[key][\"output\"][0]), wanted[0][key]\n",
    "    pass\n",
    "\n",
    "print tocni, netocni, tocni/(1.*netocni+tocni), netocni+tocni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "# code not necessarily nested in class definition\n",
    "\n",
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "    \n",
    "    vocab_size = len(dataset.sorted_chars)\n",
    "    modelRNN = RNN(hidden_size=hidden_size,sequence_length=sequence_length,learning_rate=learning_rate,vocab_size=vocab_size)\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "    num_batches = dataset.num_batches\n",
    "    batch_size = dataset.batch_size\n",
    "\n",
    "    h0 = np.zeros((batch_size, hidden_size))\n",
    "\n",
    "    average_loss = 0\n",
    "\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            batch=0\n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((batch_size, hidden_size))\n",
    "            # why do we reset the hidden state here?\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh, y_oh = None, None\n",
    "\n",
    "        # Run the recurrent network on the current batch\n",
    "        # Since we are using windows of a short length of characters,\n",
    "        # the step function should return the hidden state at the end\n",
    "        # of the unroll. You should then use that hidden state as the\n",
    "        # input for the next minibatch. In this way, we artificially\n",
    "        # preserve context between batches.\n",
    "        loss, h0 = modelRNN.step(h0, x, y)\n",
    "\n",
    "        #if batch % sample_every == 0:\n",
    "        if e and current_epoch %10 == 0:\n",
    "            print \"Epoch %s/%s, Batch %s/%s\\nCurrent batch loss = %s\" % (current_epoch, max_epochs, batch, num_batches, loss)\n",
    "            sampled_text = modelRNN.sample(dataset)\n",
    "            print \"#SAMPLED TEXT#\\n%s\\n####END###\\n\" % sampled_text\n",
    "        batch += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Batch 0/3947\n",
      "Current batch loss = 70.2882316104\n",
      "#SAMPLED TEXT#\n",
      "ERORALA:\n",
      "Is I dekn, brat Su`o a ver ssaythas lihor I faop Hut kat... yod haman.\n",
      "\n",
      "\n",
      "MEA:\n",
      "I'r mencere mamust of rane ip ar'tys till plentoe I'bong?\n",
      "\n",
      "EDDFRA:\n",
      "Sed!\n",
      "\n",
      "JEY:\n",
      "Ler.\n",
      "\n",
      "DENUYZT a ton Lare's whet hacser it ay mas?\n",
      "\n",
      "DU hat weednthe heas youe The'm to bo tre daly to by you \n",
      "####END###\n",
      "\n",
      "Epoch 10/100, Batch 0/3947\n",
      "Current batch loss = 66.4229043993\n",
      "#SAMPLED TEXT#\n",
      "BE:\n",
      "I Bellim!\n",
      "\n",
      "ORARA:\n",
      "The shoee wekm, I'r, sht dily.\n",
      "\n",
      "RARARARAUKE:\n",
      "I to?\n",
      "\n",
      "LEKE.\n",
      "\n",
      "We  ISSO3DOY:\n",
      "Ome don'll yacking lere wald ou.r I've cate o DOER:\n",
      "We abou dre pre inch they.\n",
      "\n",
      "PRITHLAKA:\n",
      "Yow thine.\n",
      "P:\n",
      "Manry dod hom toRme asour oh doll, I gom.\n",
      "\n",
      "LBANLUKAR:\n",
      "Yby Fedllert'ved he\n",
      "####END###\n",
      "\n",
      "Epoch 15/100, Batch 0/3947\n",
      "Current batch loss = 63.9546955538\n",
      "#SAMPLED TEXT#\n",
      "ARCHORE:\n",
      "Whern a st.\n",
      "\n",
      "SE:\n",
      "Wellfo Je ayou ponere we vioked.\n",
      "\n",
      "SAR.\n",
      "BOE D:\n",
      "Loevad.  Hrich offwe m your't shings alraiig lare be just armmy gobee himel waistidlr.\n",
      "\n",
      "rerilers to not se?\n",
      "\n",
      "AD:\n",
      "You dis or whin' cay know beaur matoy, that meve th's aratad swiking shis.\n",
      "\n",
      "LEKE:\n",
      "Onn'rn\n",
      "####END###\n",
      "\n",
      "Epoch 20/100, Batch 0/3947\n",
      "Current batch loss = 63.7707514998\n",
      "#SAMPLED TEXT#\n",
      "EREN:\n",
      "I lirt myizifged don thit on..\n",
      "\n",
      "BETHAY:\n",
      "Thore of pore terurid. Bet oonn ond you ackithesele, thet fur appard ofve.\n",
      "\n",
      "LARY:\n",
      "Yangother een gidsen Tre coboder where scoutale me tre of. Bet gox. Urthead tart.\n",
      "\n",
      "PANDY:\n",
      "Whass.\n",
      "\n",
      "HEETY:\n",
      "OUGHR WHURARARE:\n",
      "Yebostaer theullip to w\n",
      "####END###\n",
      "\n",
      "Epoch 25/100, Batch 0/3947\n",
      "Current batch loss = 64.2048738666\n",
      "#SAMPLED TEXT#\n",
      "ONZA:\n",
      "You youbl.\n",
      "\n",
      "LIIMAN:\n",
      "L bomerate.\n",
      "\n",
      "ERAD:\n",
      "Wead sest wort tway the fulthly ane gustre a ssestic twors.\n",
      "\n",
      "LAUSrare. maver al. Your get Thored shored . ast'm tare.\n",
      "\n",
      "LUKER WINZA:\n",
      "Gom of be Were't semous hode Evers apanche appim. The see I'm set that justie pritanaturter?\n",
      "\n",
      "LA\n",
      "####END###\n",
      "\n",
      "Epoch 30/100, Batch 0/3947\n",
      "Current batch loss = 63.9159403925\n",
      "#SAMPLED TEXT#\n",
      "AN:\n",
      "Wheashese.\n",
      "\n",
      "HOTH:\n",
      "LUKE:\n",
      ".\n",
      "\n",
      "LLEND:\n",
      "I liake!\n",
      "\n",
      "LAR AN:\n",
      "You ow.\n",
      "\n",
      "LAREIRAN:\n",
      "Y'monk shitend. emperany waing frastize?\n",
      "\n",
      "ADEBE:\n",
      "Berecherter, we agouljususall cant.  Mrichat I've Lins ybling wanfuar, its wheal a the take I cake celker Gotlry of lkib, win to cold The I'r fin was\n",
      "####END###\n",
      "\n",
      "Epoch 35/100, Batch 0/3947\n",
      "Current batch loss = 63.3908452118\n",
      "#SAMPLED TEXT#\n",
      "NKY:\n",
      "Gry Therny dakes why follaiks bout this?\n",
      "\n",
      "RETHORDNY:\n",
      "Beth!..it.\n",
      "\n",
      "HAMFECELE:\n",
      "AU mas.\n",
      "\n",
      "HEONGOR yonroneming Fer grering in it ahatull stinzy.\n",
      "\n",
      "LEOP:\n",
      "Thith.\n",
      "\n",
      "LAHRIE:\n",
      "Min.\n",
      "No sele it fast is stulik aring to nouge lat pht hew the a dis your alke?\n",
      "\n",
      "LARRIKhas that it kno!\n",
      "\n",
      "LA\n",
      "####END###\n",
      "\n",
      "Epoch 40/100, Batch 0/3947\n",
      "Current batch loss = 64.3377911698\n",
      "#SAMPLED TEXT#\n",
      "ERERICK:\n",
      "We pupper. But thking But a nove.\n",
      "\n",
      "HARA:\n",
      "It havern's tonthaw?  net, torring mys sade...  I, tyer oncon in yoniing, youy ga youlwn I lithat?\n",
      "\n",
      "DY:\n",
      "Dod you pro?\n",
      "\n",
      "TEA:\n",
      "You getrarrang netides this haqUKE:\n",
      "This no shoun't bely?\n",
      "\n",
      "TOMA:\n",
      "Your Himry mim Me I ow, iguth be af\n",
      "####END###\n",
      "\n",
      "Epoch 45/100, Batch 0/3947\n",
      "Current batch loss = 64.8448288725\n",
      "#SAMPLED TEXT#\n",
      "ESROE SAN:\n",
      "\n",
      "COOLDDY:\n",
      "Suwhit I do cot gry. Mour a waspe you havine. You hing, So.\n",
      "\n",
      "FEE:\n",
      "OHOT:\n",
      "Nhat a reanch time fre dor'ting what hoor hreve aby and aropr.\n",
      "\n",
      "GOLO:\n",
      "Gue I caie oucing al are .\n",
      "\n",
      "BEN:\n",
      "No Loids letothen' Foge a dong!. recirn.\n",
      "\n",
      "TOFFEM:\n",
      "LAURAH:\n",
      "We Well you riks wh\n",
      "####END###\n",
      "\n",
      "Epoch 50/100, Batch 0/3947\n",
      "Current batch loss = 64.2250475963\n",
      "#SAMPLED TEXT#\n",
      "E:\n",
      "It'mmy sht logk!\n",
      "\n",
      "GHEDERY:\n",
      "Lall of knot and.  Hers nowe an the me it wantranst knould mant.\n",
      "\n",
      "CHEPY:\n",
      "You afillint was ano haven thousp, right is ober.....\n",
      "Y'SGE:\n",
      "I IA:\n",
      "You quoupt farry ankfor bell any..\n",
      "\n",
      "BEY:\n",
      "The.\n",
      "\n",
      "HANG, THCKE:\n",
      "Could you any!\n",
      "\n",
      "THIUKARE:\n",
      "Hey sole they sha\n",
      "####END###\n",
      "\n",
      "Epoch 55/100, Batch 0/3947\n",
      "Current batch loss = 63.8654300701\n",
      "#SAMPLED TEXT#\n",
      "ANWE:\n",
      "Whear?\n",
      "\n",
      "LEOSNGE:\n",
      "Te? Juch is ast Lasableff hill on...  frone, D8ITNE:\n",
      "Houldy will toll, bush.\n",
      "\n",
      "BEG:\n",
      "Tady.\n",
      "\n",
      "GERGENNDO:\n",
      "Whatten't they the comar.\n",
      "\n",
      "MR. L:\n",
      "Yet hy what M?\n",
      "\n",
      "LUKE:\n",
      "Sonewiget canrimy set we gurabou\n",
      "\n",
      " care. An woar Pascupare as do I dnallat.\n",
      "\n",
      "FREY:\n",
      "You shod N\n",
      "####END###\n",
      "\n",
      "Epoch 60/100, Batch 0/3947\n",
      "Current batch loss = 63.9449559861\n",
      "#SAMPLED TEXT#\n",
      "ASSARE:\n",
      "She colle!\n",
      "\n",
      "KOAN:\n",
      "Y:\n",
      "THY:\n",
      "SE:\n",
      "Whinack is tome ond that pe I or, Gatifuthess.\n",
      "\n",
      "MADDY:\n",
      "You somy with the quourne. Do Lunked Gow af you for there an the Ee holl here no, turquet on my, foJut gook jut thiss.\n",
      "\n",
      "GOOUDY:\n",
      "Weally noce. What dean morend in mall attom hys aple\n",
      "####END###\n",
      "\n",
      "Epoch 65/100, Batch 0/3947\n",
      "Current batch loss = 64.4480148906\n",
      "#SAMPLED TEXT#\n",
      "ARARE:\n",
      "We seartter whinder an hore ligs here thad.\n",
      "\n",
      "HEE:\n",
      "The kny I wars for loncure thenk goe evelres his!..\n",
      "\n",
      "SIM ICAN:\n",
      "Now hat go hat. Be falls do. You got once 'ee goiboded fulk al hire We goond.\n",
      "\n",
      "HORMMD:\n",
      "Thas nowtlonken.\n",
      "\n",
      "HEEN:\n",
      "Grorake..\n",
      "\n",
      "LETER:\n",
      "Were'l. well oul its wan\n",
      "####END###\n",
      "\n",
      "Epoch 70/100, Batch 0/3947\n",
      "Current batch loss = 64.5493830418\n",
      "#SAMPLED TEXT#\n",
      "ESDINK:\n",
      "Thun Ei delayew up.\n",
      "\n",
      "CYNZONER:\n",
      "She shing?\n",
      "\n",
      "HANT:\n",
      "No. I dove hy ma kid. mou ch ups.\n",
      "\n",
      "JIGHAN:\n",
      "VEON:\n",
      "On lin on frig is go Jee there lid ale to up.\n",
      "\n",
      "LONCOH:\n",
      "I ke'n up oldy come I'm I knyo this onifucke did it my is Frey.\n",
      "\n",
      "Guko not down.\n",
      "\n",
      "HAN:\n",
      "You go somk Ceall gected h\n",
      "####END###\n",
      "\n",
      "Epoch 75/100, Batch 0/3947\n",
      "Current batch loss = 64.6355191154\n",
      "#SAMPLED TEXT#\n",
      "E:\n",
      "I quentcher Dins.\n",
      "\n",
      "LAKE:\n",
      "He'pelworthis.\n",
      "\n",
      "FAKA:\n",
      "It's some there.\n",
      "\n",
      "LUKARUETENEDY:\n",
      "Ite know fall about bare be wathea buctr. Th.\n",
      "\n",
      "DUKE:\n",
      "A it?\n",
      "\n",
      "LAIDARAS:\n",
      "What is it got to here out a I coures held do cout then't?\n",
      "\n",
      "GESIMA:\n",
      "Whis diad.\n",
      "\n",
      "DORE:\n",
      "Liked.\n",
      "\n",
      "VENZOSEY:\n",
      "Nrut agore, he n\n",
      "####END###\n",
      "\n",
      "Epoch 80/100, Batch 0/3947\n",
      "Current batch loss = 64.6707227731\n",
      "#SAMPLED TEXT#\n",
      "ANDY:\n",
      "Gothere you and mifg thing?\n",
      "\n",
      "LUKARBEN:\n",
      "We Goog.  Hess our prenchifie, mes of on the ire thin't ussting, you? I..\n",
      "\n",
      "LEAN:\n",
      "I don'teltie sast you dogiusankizsas odrarke, actill CEETISO:\n",
      "Herife chatied a his hipe.\n",
      "\n",
      "HANA:\n",
      "She fue, him.  of Merlersstever just Soy!\n",
      "\n",
      "DAN:\n",
      "I'm\n",
      "####END###\n",
      "\n",
      "Epoch 85/100, Batch 0/3947\n",
      "Current batch loss = 65.9609776518\n",
      "#SAMPLED TEXT#\n",
      "WOL:\n",
      "You seonef taking.\n",
      "\n",
      "JICK:\n",
      "Nom know ove there histed it's you!\n",
      "\n",
      "GEADE:\n",
      "He?\n",
      "\n",
      "RARY:\n",
      "Sore as bean hat you deren't you and ander gaor hertire furtht.\n",
      "\n",
      "PERIUKE:\n",
      "Luereves tidaking frablin it lif.\n",
      "\n",
      "CHE ANICK:\n",
      "Here eproone the the now Is CEN. So Thow ity, an ply.\n",
      "\n",
      "MRE:\n",
      "I pritw\n",
      "####END###\n",
      "\n",
      "Epoch 90/100, Batch 0/3947\n",
      "Current batch loss = 65.3902656613\n",
      "#SAMPLED TEXT#\n",
      "ARVEUREN:\n",
      "I he wonton whiter's exhathinna gill be guontaum MAVERESNKTETO:\n",
      "Ge ole Comm I'm know and stistarsty mexp.\n",
      "\n",
      "MAMIGHIEN:\n",
      "Nogerereer. JoFRDA4ERSOR:\n",
      "Bh, be ut intre goor.\n",
      ". Hly dam of wisn't for goe's wery nod a here Pat jukigike is geseort bush Ko hore sow moped sers\n",
      "####END###\n",
      "\n",
      "Epoch 95/100, Batch 0/3947\n",
      "Current batch loss = 65.1307480809\n",
      "#SAMPLED TEXT#\n",
      "HE MIMMANLOLAR:\n",
      "It's on. Borrame rake?\n",
      "\n",
      "Lukink so to rise, Yof This the con't.\n",
      "\n",
      "BENIE:\n",
      "Ton't to hap your and you intsachih apperror you don't was hiishe of Meranry mer.  I wan con trip. .\n",
      "\n",
      "MATET:\n",
      "He had the you, you n here.\n",
      "\n",
      "LANDAR:\n",
      "There Mittry of and hal, and of MCKA:\n",
      "Su\n",
      "####END###\n",
      "\n",
      "Epoch 100/100, Batch 0/3947\n",
      "Current batch loss = 65.7836925926\n",
      "#SAMPLED TEXT#\n",
      "OLE:\n",
      "Jue nabliad I hey hat mores. to of here.  is you, wand woer mufterr, catlan, rolah!\n",
      "\n",
      "NAY.. What?\n",
      "\n",
      "TRA:\n",
      "Non guer got the orgothitien it. Af to, are guar...  I I vere when!\n",
      "\n",
      "DREK:\n",
      "Thos in caulther soom. I'm obed.\n",
      "\n",
      "VAREN:\n",
      "Come is.\n",
      "\n",
      "LEUGOD:\n",
      "The Trusee cand go's comed Foul\n",
      "####END###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "D = Dataset(5,30)\n",
    "D.preprocess(\"input.txt\")\n",
    "D.create_minibatches()\n",
    "run_language_model(D,100,sample_every=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Batch 0/3947\n",
      "Current batch loss = 72.159582649\n",
      "#SAMPLED TEXT#\n",
      "AI CHou drerife of forn sere or to me en yousn's we it.\n",
      "\n",
      "CHA:\n",
      "Id.\n",
      "\n",
      "EDEE:\n",
      "Soust nougn, thatoonger. Hanurg to you YGOOD:\n",
      "POND:\n",
      "I gabec not kiog iny itbeing oul be whrver it thus to lBhe Teksea socking!.\n",
      "\n",
      "DONIE:\n",
      "Wherer. Therigh beldir if aoyick.!  hice fo!\n",
      "\n",
      "I cad to thasen't \n",
      "####END###\n",
      "\n",
      "Epoch 20/200, Batch 0/3947\n",
      "Current batch loss = 70.6747552316\n",
      "#SAMPLED TEXT#\n",
      "HAN:\n",
      "Bre the is you there.  I whave seugr any!  Pored mamm the  to periom 're\n",
      "HEN:\n",
      "Thaectsl verg tant,  it. Trat is wabre thie cren.  dewr sow, foron if a rey of as all don'r mardel hald. W... Yesty cite\n",
      "\n",
      "VARE hatee ads right well sour if all aburnowss, ued Vall wablide.\n",
      "\n",
      "\n",
      "####END###\n",
      "\n",
      "Epoch 30/200, Batch 0/3947\n",
      "Current batch loss = 68.0875230263\n",
      "#SAMPLED TEXT#\n",
      "RELERICK\n",
      "TETA:\n",
      "I'm neutryou't carupt. not mick only rame thin't the muct quet.\n",
      "\n",
      "FUNGO:\n",
      "Thowh.  You's him, puch ove?\n",
      "\n",
      "CILEckotagr. Evebbl stted a pra live ain do.\n",
      "\n",
      "LAVERCHICH:\n",
      "Howh.\n",
      "\n",
      "LARAN:\n",
      "Ro, that you ksone sty fot's nowe!\n",
      "\n",
      "CHANKO:\n",
      "Ohm may dor buol we lasfing. Lit you?\n",
      "\n",
      "L\n",
      "####END###\n",
      "\n",
      "Epoch 40/200, Batch 0/3947\n",
      "Current batch loss = 65.8261016842\n",
      "#SAMPLED TEXT#\n",
      "AX:\n",
      "Le cant wat lignde it, There hassh, ARSilsin ang at thoon, mapt sedake berexonsy dor pealvit's rere fre the deed we. it, I'm chert beiny.  Yic thand tales osting, don't garked I ant there doty in, my talet'ne gecrelles thentaty or the man. Mruck know.\n",
      "\n",
      "BLAOD:\n",
      "On't itsi\n",
      "####END###\n",
      "\n",
      "Epoch 50/200, Batch 0/3947\n",
      "Current batch loss = 66.6434450614\n",
      "#SAMPLED TEXT#\n",
      "ELER:\n",
      "When ok, bace.  our to home f reaffor mopatmon't neverred to waysth your sithave ore oust it efartoily deretse Hay, you up.\n",
      "\n",
      "LEIC:\n",
      "Arery need. noider, ust worh do these thounden we grovens never noold ing nomer you?\n",
      "\n",
      "HOT:\n",
      "Whuse 400 all why. He is off ored seliks like\n",
      "####END###\n",
      "\n",
      "Epoch 60/200, Batch 0/3947\n",
      "Current batch loss = 67.2852354332\n",
      "#SAMPLED TEXT#\n",
      "ANDD:\n",
      "Your tllishow the wyy ir fiett wis a looucknee Is your the prisied Wor Look?\n",
      "\n",
      "LUKE:\n",
      "I dorits a of I diest wy. You reith are like els toussh toifperfoe a shere a sam, you. Leat pareat  thern now and. Then flett it mear hoinyg of it a'ss won't Sartedne yourar have kids\n",
      "####END###\n",
      "\n",
      "Epoch 70/200, Batch 0/3947\n",
      "Current batch loss = 66.1606231393\n",
      "#SAMPLED TEXT#\n",
      "HRECREXY:\n",
      "Yo boppeakongofpeffpppyeafpppededpprysywywdy'vederssyeveeffpeffppponpexpppyboy'merexpvedy, sppewgexpy..\n",
      "\n",
      "YRORLIE:\n",
      "JYop buffedpytswheap whofedshepperffepperqexppperbyefeaxspebpers?\n",
      "\n",
      "IVEK00000007\n",
      "\n",
      "\n",
      "JI\n",
      "RGOURYOEREX:\n",
      "JIENDREXY:\n",
      "KXheliveppeckeseffffhepbexppbyo,000?\n",
      "\n",
      "\n",
      "N\n",
      "####END###\n",
      "\n",
      "Epoch 80/200, Batch 0/3947\n",
      "Current batch loss = 65.5959199929\n",
      "#SAMPLED TEXT#\n",
      "RRY:\n",
      "RIK0000000\n",
      "ISUREIVEIEXICK:\n",
      "JO000000000000707000007 IMh'\n",
      "TREDEXY'shy'tsteppveppeppepppbexppodspern ffusffppppewstepewseyfere\n",
      "Ppvyay dsky'vedy?\n",
      "\n",
      "HAN:\n",
      "JIELEY:\n",
      "h, jiffexeppewpppypursexwesepsentelfelfandrenefe.\n",
      "\n",
      "Yor,0\n",
      "\n",
      "HENREDY:\n",
      "KXoy'verepppppedyberrandyeereberfurpyaxpppyst\n",
      "####END###\n",
      "\n",
      "Epoch 90/200, Batch 0/3947\n",
      "Current batch loss = 66.5578449174\n",
      "#SAMPLED TEXT#\n",
      "ENEPY'ndys.\n",
      "HINGENEK:\n",
      "I'MRNEKExtedy0000000997000003000000000300000700000000030000000000000009000377\n",
      "\n",
      "VEFRRI'ys he,000000000000700000000009700379000070000000000008034000070000000000000300030000000007000000000000000000030000000303400000000300000000000005000000000007030707070\n",
      "####END###\n",
      "\n",
      "Epoch 100/200, Batch 0/3947\n",
      "Current batch loss = 66.3275727762\n",
      "#SAMPLED TEXT#\n",
      "EIA:\n",
      "Forer.\n",
      "\n",
      "JONZO:\n",
      "No. Jefling right worett, CINTOR:\n",
      "Noces, wy that kin to west on Det He.\n",
      "\n",
      "GONZO:\n",
      "Codng..... and the coby the needs of and you trame her thene the in bust dids? Westayss some tes the pawited Thie thing wis bir ckied! I can't ven Duss you condoway, thrembe\n",
      "####END###\n",
      "\n",
      "Epoch 110/200, Batch 0/3947\n",
      "Current batch loss = 65.9077841652\n",
      "#SAMPLED TEXT#\n",
      "RIELAH:\n",
      "Pad...  Carkngs you.\n",
      "\n",
      "LEIA:\n",
      "Chat home.\n",
      "\n",
      "DELIA:\n",
      "Oh, it's. Lorr. Hed. Mr. ... stablere In  of. There right what finctucaron't me burnigros.  I'm car righ birry you dun foheray mass me?\n",
      "\n",
      "LUKE:\n",
      "RIT YI LUG AN  fues get Frank timmaning!.\n",
      "\n",
      "HAN:\n",
      "Who to ou about what?\n",
      "\n",
      "SAND\n",
      "####END###\n",
      "\n",
      "Epoch 120/200, Batch 0/3947\n",
      "Current batch loss = 65.4364376409\n",
      "#SAMPLED TEXT#\n",
      "ANNIE:\n",
      "You knatout rested us then ousing a with about whokay.\n",
      "\n",
      "HAN:\n",
      "Not she you.\n",
      "\n",
      "LAURA:\n",
      "What's you for wergag the peatsing y:\n",
      "Tome ving Leallonear where uns I think whfromsil.\n",
      "\n",
      "GONZO:\n",
      "Yeaors beer.\n",
      "\n",
      "NARAH:\n",
      "So list?\n",
      "\n",
      "MES DA!...K.DALMOE:\n",
      "No.\n",
      "\n",
      "CONGE:\n",
      "Moffe.\n",
      "\n",
      "HAN:\n",
      "Wou you go s\n",
      "####END###\n",
      "\n",
      "Epoch 130/200, Batch 0/3947\n",
      "Current batch loss = 64.8178347656\n",
      "#SAMPLED TEXT#\n",
      "UKE:\n",
      "Lut outtid don't eeble Leaddy righbliden. He you?\n",
      "\n",
      "LUKE:\n",
      "It's I'd that thiskcke!\n",
      "\n",
      "LANDO:\n",
      "Come, I'cked not the hardye know them you fine with you lamed denembery gahow a jupped Lrog threout that.\n",
      "\n",
      "LUKE:\n",
      "No.\n",
      "\n",
      "HAN:\n",
      "Well.\n",
      "\n",
      "LEIA:\n",
      "You've beek of about the neid?\n",
      "\n",
      "LEIA:\n",
      "Her y\n",
      "####END###\n",
      "\n",
      "Epoch 140/200, Batch 0/3947\n",
      "Current batch loss = 64.2126286774\n",
      "#SAMPLED TEXT#\n",
      "ANDO:\n",
      "Ind be leal tay.\n",
      "\n",
      "HAN:\n",
      "Orame had If I'm Ha this is anoursed you get I hens weakes... thing! What forf seale insot has to the ste suching. in mustion. Ared hentive, trieter clown to Anse, is stladly here you, I wa don the in there's Lill?\n",
      "\n",
      "LEIA:\n",
      "Iwoid!\n",
      "\n",
      "RAOD:\n",
      "And. the\n",
      "####END###\n",
      "\n",
      "Epoch 150/200, Batch 0/3947\n",
      "Current batch loss = 64.4109719628\n",
      "#SAMPLED TEXT#\n",
      "ERME:\n",
      "Everned?\n",
      "\n",
      "BINLER:\n",
      "ANIE:\n",
      "Who traild.\n",
      "\n",
      "LAURA:\n",
      "What filut!\n",
      "\n",
      "LUKE:\n",
      "Theenly a problea for rime you found with thit's to her, your moreffer. . I onous.\n",
      "\n",
      "HAN:\n",
      "I'm about upsen the tiknon stare.  Harryss my how shoooe frumse id were ere, Loura . The me... what case frulseathe\n",
      "####END###\n",
      "\n",
      "Epoch 160/200, Batch 0/3947\n",
      "Current batch loss = 64.2637905931\n",
      "#SAMPLED TEXT#\n",
      "DDO:\n",
      "You.\n",
      "\n",
      "LEIA:\n",
      "Then't drounder the shouh\n",
      "What's ring inlike on do. This the fuckald yon one suioke a les have a warling there work?\n",
      "\n",
      "LEIA:\n",
      "Y:\n",
      "Where hes.\n",
      "\n",
      "LANDOND:\n",
      "Peald at an noblisn it some out?\n",
      "\n",
      "TEREN:\n",
      "CHORCHLEE:\n",
      "Ore fooget, filled know.  And dong entor the here it!\n",
      "\n",
      "L\n",
      "####END###\n",
      "\n",
      "Epoch 170/200, Batch 0/3947\n",
      "Current batch loss = 63.9425512294\n",
      "#SAMPLED TEXT#\n",
      "EIA:\n",
      "Love from lellfor you... this.  jop Jopern.\n",
      "\n",
      "DA:\n",
      "My didment.\n",
      "\n",
      "LANDO:\n",
      "I'll offe beinge you dreation. Alppatan Hey. Jordaridal you a think infortureasd?!\n",
      "\n",
      "LEIA:\n",
      "You're heout fooke. Then't doing, go. sime the Calk?\n",
      "\n",
      "LAND:\n",
      "He ent know wikh, loys.\n",
      "\n",
      "TOHWIE:\n",
      "It was atull us \n",
      "####END###\n",
      "\n",
      "Epoch 180/200, Batch 0/3947\n",
      "Current batch loss = 63.4414953086\n",
      "#SAMPLED TEXT#\n",
      "ETMACY:\n",
      "But litter the are Dors I emplamentws many my the will cremplarton my on prweet it whanfthe meem...  you speey bucking offorrake, but that to the coping to put no.\n",
      "\n",
      "VARIA:\n",
      "Ari, on where ingo the aith.\n",
      "\n",
      "LUNN:\n",
      "Let... yer Dear.  Yefor fily nappe dadaber tay beine!\n",
      "\n",
      "RE\n",
      "####END###\n",
      "\n",
      "Epoch 190/200, Batch 0/3947\n",
      "Current batch loss = 64.7722323726\n",
      "#SAMPLED TEXT#\n",
      "UKE:\n",
      "I don I'm elod be amere a the bald about seem of this suppleot hered there? Ohe'll hey, a where ase as the siet foe  inulay?\n",
      "\n",
      "HOD:\n",
      "He's with thry.\n",
      "\n",
      "LUKE:\n",
      "The out you.\n",
      "\n",
      "HANp caurase that hold never to the didlywathy wall belle repult need exife...\n",
      "\n",
      "COOBEL:\n",
      "I've ba loo.\n",
      "####END###\n",
      "\n",
      "Epoch 200/200, Batch 0/3947\n",
      "Current batch loss = 65.7236123698\n",
      "#SAMPLED TEXT#\n",
      "EIA:\n",
      "Pod wastevers tures not.\n",
      "\n",
      "LAND:\n",
      "Kistm graid.  Viknose is have the breaced ok you king, is woll' the want 20. Nwaillk, Re's lmone.\n",
      "\n",
      "COCERR:\n",
      ", when take would chapper Stuen't the dard se..  happateats the bick a do prun hintever leartithe ago I don't othing a porm.\n",
      "\n",
      "LEI\n",
      "####END###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "D = Dataset(5,30)\n",
    "D.preprocess(\"input.txt\")\n",
    "D.create_minibatches()\n",
    "run_language_model(D,200,sample_every=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Batch 0/1973\n",
      "Current batch loss = 103.752281838\n",
      "#SAMPLED TEXT#\n",
      ". rII,DsIrtet  \n",
      " rnd i\n",
      "\n",
      " ea o tt'oae sIEnoL Ve\n",
      " uhotAT..OoO\n",
      "Ltrehfsf:Wmgtw.e?:r\n",
      "lpitb\n",
      "C. s yatenRediw hthAeiE\n",
      "is :fmsu tkw \n",
      "rsn asarou?oodi  .eIr,WA\n",
      "enf hec.niE  oh\n",
      "dOdr\n",
      "ruerEBspehtAoc\n",
      "pk  iaoLlrI\n",
      "IdeHX  le.tDvpe i  id fhyty itsau\n",
      "wte.h.Ant  P\n",
      "r evsUrau . alwf:Nh  LdIuerl:\n",
      "####END###\n",
      "\n",
      "Epoch 20/200, Batch 0/1973\n",
      "Current batch loss = 87.3839139533\n",
      "#SAMPLED TEXT#\n",
      "I dv iturrrit heuul h NLto trrlo g o  RDBh o wPWEWDELDDet,icriLOo mhf.\n",
      "\n",
      "E:\n",
      "\n",
      "\n",
      "COI:\n",
      "io skir k tanwANTm Iv.\n",
      "L\n",
      "L:\n",
      "at tnisseo ve nfi.\n",
      "\n",
      "\n",
      ":.\n",
      "\n",
      "Iw h r tpeh.or tpiiglidst l ldee e rdin tfareaago ag iyl Oa.\n",
      "ee hic NIo sd.\n",
      "!\n",
      "eyee z miyto .VAAfhuerwauinsriBEONad!.YourYnh,ris ih h.a'nee\n",
      "####END###\n",
      "\n",
      "Epoch 30/200, Batch 0/1973\n",
      "Current batch loss = 84.3586118822\n",
      "#SAMPLED TEXT#\n",
      "AolatCDEI,V:\n",
      "Ph nwde,  to eysoripr  yo l at0 yhfers me bk gho heu,enugsas wFTI  wH:\n",
      "ayy.\n",
      "\n",
      "LDI: horeas\n",
      "\n",
      "I sninne?\n",
      "\n",
      "H:\n",
      "er ttitB 'nse'leotodyr af.e eedsr at lin o reu hore me tSBkoelr oel s.\n",
      " Iataned se'n.\n",
      "il e h.errs oe to   LAUSKDUoe'el'ry!\n",
      "h\n",
      " RLAE:\n",
      "NF:\n",
      "Ihtaor.. LTLh hm as \n",
      "####END###\n",
      "\n",
      "Epoch 40/200, Batch 0/1973\n",
      "Current batch loss = 83.6410928402\n",
      "#SAMPLED TEXT#\n",
      "h,e b'sag te wonir?o mhe hils e'rinrer! e sr.LLU:\n",
      "Ago.\n",
      " A LRFtWodel an one,gacm e sif gol'ru s to oen teI!\n",
      ":\n",
      "\n",
      "IHARJo?\n",
      "\n",
      ": hrur mTU:\n",
      "ev. Iyh mecauge w s. m Iganl taftomaufteueneet m?\n",
      ".\n",
      "h sYhoda t AIoev cwose ee ctotu w tOH:\n",
      "\n",
      "NTOP:\n",
      "e gangesimt feetseepat doff o kbede'r ie'uuv\n",
      "####END###\n",
      "\n",
      "Epoch 50/200, Batch 0/1973\n",
      "Current batch loss = 81.4467659353\n",
      "#SAMPLED TEXT#\n",
      "LONDeitet bt ols!TUNougt itet oy.eigI ereeorld uhemh? T atosnkur orinadell's aly bu ayare dovacan n AH:\n",
      "\n",
      "E:\n",
      "\n",
      "LLINW:\n",
      "\n",
      ": Bos ideru enu'riniwataeb.\n",
      "\n",
      "V:\n",
      "\n",
      "LE:\n",
      "\n",
      ":\n",
      "\n",
      "LLANAe oet be bo d'r.\n",
      "e we.\n",
      "o nhasekolrilwey NL Bhiroenow it ois b eeree tish ep.\n",
      "Doeut yar..\n",
      "\n",
      "TNAE:\n",
      "\n",
      "Sor ea dacirf\n",
      "####END###\n",
      "\n",
      "Epoch 60/200, Batch 0/1973\n",
      "Current batch loss = 78.9765200122\n",
      "#SAMPLED TEXT#\n",
      "LLAIT:\n",
      "\n",
      "BO:\n",
      " kog'n rinee whataru mehu fe ssag ofue e serd soes wag hipatro eroru r fape inls evge nomt Qe r pin kud ev ht Ahenicchy od. oueto y e horrmares yhm?\n",
      "Le. hWh tanlen wrims to,, blhiy it t dd, me. o o alg.oes'nrdech in gt at anatrad hr.erid has weled ytes Bowles l\n",
      "####END###\n",
      "\n",
      "Epoch 70/200, Batch 0/1973\n",
      "Current batch loss = 79.7081705994\n",
      "#SAMPLED TEXT#\n",
      "EAALUIicl.\n",
      "\n",
      "HA:\n",
      "Lino'y m.e d oun w fngicy Iin Uig?\n",
      ".\n",
      ". b,.\n",
      "W Y Oint athetdol be s theem!et I e LELA:\n",
      "Tou nht kutt nidyoreurw kute.\n",
      "in.\n",
      "\n",
      "E:\n",
      ".hesd'n lod couennrtr :\n",
      "Yolse.\n",
      "\n",
      "ao lir w'nunlipl ire unos..\n",
      "Tamedelepy as. ol ur! oases he m?\n",
      "\n",
      "N:\n",
      "\n",
      "EA:\n",
      "\n",
      "Iow chircte?\n",
      "OLunw aret tor ca\n",
      "####END###\n",
      "\n",
      "Epoch 80/200, Batch 0/1973\n",
      "Current batch loss = 79.4095704323\n",
      "#SAMPLED TEXT#\n",
      ":\n",
      "NHNII I N:\n",
      "Lo iacos m es  Kesrr.'krlyiarg halit qrMad dhterd buisd un  he' inurw at.e h douy tPHobwow ps ot bune s ke odnre wb Irc!\n",
      "\n",
      "DAALOKO:\n",
      "OSHFABI:\n",
      ".\n",
      "\n",
      "EERDE:\n",
      "Ah.\n",
      "\n",
      "LCNNAALLNDUAL:\n",
      "e'ot Dliou y lp iiot tsen dhon've m b.epy yt he't ou ore yh thA fom x, yhir.\n",
      "\n",
      "ELEM:\n",
      "NDNou \n",
      "####END###\n",
      "\n",
      "Epoch 90/200, Batch 0/1973\n",
      "Current batch loss = 75.7038826721\n",
      "#SAMPLED TEXT#\n",
      "AVLeao someile shd ttof.\n",
      "\n",
      "T:\n",
      "HKA:\n",
      "Bit thwaeche Skun syost!\n",
      "Yont.h ove 0arot dae fpannot OIit pVinl kiwl. fot meyot\n",
      "\n",
      "DAun?.\n",
      "\n",
      ":\n",
      "\n",
      "E:\n",
      "Ohen?\n",
      "\n",
      "ONIAr..'s ou the fern b'let gt..he. yhupg ton it.\n",
      "Wany aliad chch\n",
      "MABAPLVLDE:\n",
      "Lnato.\n",
      "Badr b?\n",
      "\n",
      ":\n",
      "Scow. Mteus wicousert hiy..\n",
      "\n",
      "Slod. h\n",
      "Ie \n",
      "####END###\n",
      "\n",
      "Epoch 100/200, Batch 0/1973\n",
      "Current batch loss = 73.3435047471\n",
      "#SAMPLED TEXT#\n",
      "ROEA:\n",
      "Dlyhie op e!.\n",
      "\n",
      "IORLorildd. WWAI ac esy LENPEISK:\n",
      "\n",
      "LWLESEOGUNPVDuneer.\n",
      "\n",
      "OGKAT:\n",
      "Wever burorge pllune. :\n",
      "B.\n",
      "\n",
      ":\n",
      "\n",
      "LKLIULIreouvelu tol, toiron weort amefeshe onevad, cabit mer npw love nju aos et peur ipdor. Do ctola. ake wgaw ro thaco, ate shJlfre y ann'sen?\n",
      "ENhht hep ase\n",
      "####END###\n",
      "\n",
      "Epoch 110/200, Batch 0/1973\n",
      "Current batch loss = 72.9354484178\n",
      "#SAMPLED TEXT#\n",
      "CNWDIGO:\n",
      "Huioquk len't.\n",
      "\n",
      "Yousdg.\n",
      "\n",
      "DELID:\n",
      "Hog.\n",
      "\n",
      "AAPReio  ytouona urro e tour wGallt he axu mular talfs toner ve mgowocasallr in!\n",
      "\n",
      "AMLECENLIDII.\n",
      "\n",
      "TORDESBAUENHALEN:\n",
      "Wave. Mo kondto wis nOLee mto Ho fodts cosd's sa ju st I anor ou Ohelsig. BANicl  I angedlbe bikonart ou touu t\n",
      "####END###\n",
      "\n",
      "Epoch 120/200, Batch 0/1973\n",
      "Current batch loss = 72.7071310372\n",
      "#SAMPLED TEXT#\n",
      "GAN:\n",
      "\n",
      "D\n",
      "E:\n",
      "Li Atwanetyhare, iset fang. e breng! Sodon htio Wh!. BE:\n",
      "I ot isa uyou rinwed bid feud tege.\n",
      "\n",
      "ADALTAN:\n",
      "Hins. TOhivisouus verve Mlisa tsy or ket Seu ffas flods  montt yhe lpe..\n",
      "\n",
      "IRERIeg.\n",
      "\n",
      "RO:\n",
      "eye to, kincis hsogyoumoule he th tdoplut weise..\n",
      "\n",
      ":\n",
      "Yog ave's yhitt th\n",
      "####END###\n",
      "\n",
      "Epoch 130/200, Batch 0/1973\n",
      "Current batch loss = 71.7141957392\n",
      "#SAMPLED TEXT#\n",
      "ADRENAR:\n",
      "Ahinnn?\n",
      "\n",
      "LELLETE:\n",
      "\n",
      "LKTEDKFIONIN:\n",
      "So rpa we, ive Pras the's, fuke obet ke!.\n",
      "\n",
      "ELVO:\n",
      "Amimua, wha' thed le. he koltt Se rinep uera I th bonienhert?\n",
      "Leoulmave ritor Is o mabep?\n",
      "\n",
      "EWO:\n",
      "homdet chinbs, int emackep, gyr Te kek ira thwdert.\n",
      "\n",
      "HOUHong.\n",
      "\n",
      "DLEREIP Thala's Laret o\n",
      "####END###\n",
      "\n",
      "Epoch 140/200, Batch 0/1973\n",
      "Current batch loss = 70.9533581807\n",
      "#SAMPLED TEXT#\n",
      "NOE:\n",
      "Onot. Yods K:\n",
      "\n",
      "DILEUDE:\n",
      "MLoot it fit sout therl's...\n",
      "\n",
      "LAUJYADARK:\n",
      "La'ten colgecm te thinprumow hunge fvel tham A delk thed r. Hee keullat obis anis'l.\n",
      "\n",
      "ARTHALREN:\n",
      "Dridy.. we?\n",
      "\n",
      "HE:\n",
      "O.. Lad wo wI dent ar. w. E:\n",
      "Lomer krand ade kan, we wolaninr ge the bey me meoretodt ur\n",
      "####END###\n",
      "\n",
      "Epoch 150/200, Batch 0/1973\n",
      "Current batch loss = 72.3908663426\n",
      "#SAMPLED TEXT#\n",
      "RAA:..\n",
      "\n",
      "FDELAO:\n",
      "I've co tou bor, souk wipg u at at LAAMO:\n",
      "Choy w frat.  rerins Ewou ot I't then meu u mifu nothiverin yhese beul, Whemi't fe me't an dinekiit. LED:\n",
      "I wou col fes lope feofiy bre soul ser in yOnour in he koushes.\n",
      "\n",
      "AAThithis'lrd!\n",
      "\n",
      "RDAE:\n",
      "Ye,.. ye. hlg. hincige\n",
      "####END###\n",
      "\n",
      "Epoch 160/200, Batch 0/1973\n",
      "Current batch loss = 69.8471173749\n",
      "#SAMPLED TEXT#\n",
      "TDONRA:\n",
      "\n",
      "BEDE:\n",
      "If Peon ank, wonyalrighene cis ot.\n",
      "\n",
      "RDOHEN:\n",
      "He cet R0Loyhes we hed  ure a?\n",
      "\n",
      "DHNORTK:\n",
      "A Ye..\n",
      "\n",
      "LDet jid kreve nou A toud, Jeritet?\n",
      "\n",
      "EMNRRout tory youllre tpang dind veve?.Th in.\n",
      "\n",
      "LLIVENE:\n",
      "\n",
      "MRIA:\n",
      "MA Yoboufe cao her shanhe wa. Whas thol'thl, SIs jat..  waof endt\n",
      "####END###\n",
      "\n",
      "Epoch 170/200, Batch 0/1973\n",
      "Current batch loss = 67.8308261895\n",
      "#SAMPLED TEXT#\n",
      "ORDD:\n",
      "Ceas.Y:\n",
      "Yoon's lpluboun lemack? hit..\n",
      "\n",
      "LESSER:\n",
      "Sor, eve gereelll  in the nhu torrou rit may.\n",
      "\n",
      "LAFREIA RARADER:\n",
      "Wher wame wty goiver.\n",
      "\n",
      "KRECHAR:\n",
      "Is. I Bivad't's do've hof tidod. Beu..\n",
      "\n",
      "IRESLAR.\n",
      "E DOI ME:\n",
      "Thy e than. hu'med er gou st.\n",
      "\n",
      "EMRDY.., mo soll's o nanle.\n",
      "\n",
      "DIME:\n",
      "####END###\n",
      "\n",
      "Epoch 180/200, Batch 0/1973\n",
      "Current batch loss = 67.3783535681\n",
      "#SAMPLED TEXT#\n",
      "HOVAND:\n",
      "Whers thimore camp, that thken.\n",
      "\n",
      "JOFMO:\n",
      "Mmithe we, tomce irio'n mis ou, malk rie. 10'ke Zeg siext com gat hge mekt ndie thet I'sb o ther ballr touve dinght to lovk, ing. LOE:\n",
      "Wighthhas buy anly, Busr onsidrell, He hitur nime you an thkrag fom ther halg ust co So fu\n",
      "####END###\n",
      "\n",
      "Epoch 190/200, Batch 0/1973\n",
      "Current batch loss = 67.5585580801\n",
      "#SAMPLED TEXT#\n",
      "ANANES:\n",
      "I dim ounly lang o lerst Luy if comer pellcerm.\n",
      "\n",
      "TRofom ner ling to le orer.\n",
      "\n",
      "PIAN:\n",
      "Misao erd. Be puhey thef ast whingeve bures th. Yo!  thacke nho do.\n",
      "\n",
      "LMINMAM:\n",
      "Yong. !\n",
      "\n",
      "LUMAN:\n",
      "The mapst.\n",
      "\n",
      "LEFEPFUAN:\n",
      "Payl?\n",
      "\n",
      "Y:\n",
      "Eare  oll 'tme be? O a it Popar ofscond ums dhetitarld\n",
      "####END###\n",
      "\n",
      "Epoch 200/200, Batch 0/1973\n",
      "Current batch loss = 63.6185778511\n",
      "#SAMPLED TEXT#\n",
      "AVEREE:\n",
      "Mang gotincr soit.\n",
      "\n",
      "CARECK:\n",
      "Got thifre be's towhat's you you?\n",
      "\n",
      "EDDY:\n",
      "Rot's forikiplasey, song?\n",
      "\n",
      "PRIE:\n",
      "Pa?! heb.\n",
      "\n",
      "JELEY:\n",
      "LY:\n",
      " for not orent. you, Sing nocy.\n",
      "\n",
      "HIN:\n",
      "Preack bsge yoing? Semrece cicert war.. mbatks the she!\n",
      "\n",
      "LEJIDNGODKIMR:\n",
      "Whis. Ang lighig the gucking.\n",
      "\n",
      "\n",
      "####END###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "D = Dataset(10,30)\n",
    "D.preprocess(\"input.txt\")\n",
    "D.create_minibatches()\n",
    "run_language_model(D,200,sample_every=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Batch 0/3947\n",
      "Current batch loss = 73.8030609792\n",
      "#SAMPLED TEXT#\n",
      "VDZE:\n",
      "Sseelekere theryend .. Lade Ot s Ziy yorg pas, won,, med.\n",
      "\n",
      "Rid thLitlak hls'ce uns musld shermeve. Lasonthave ot. we wtisinneiol eh ko se tpicull, here bugt,. Dal c Llbeadpret Uca urtds'reoue.. Phy ylg my the ahg nous yolivol heidurgy thiced?\n",
      "\n",
      "as sas at tanee tin ton\n",
      "####END###\n",
      "\n",
      "Epoch 20/200, Batch 0/3947\n",
      "Current batch loss = 65.7453870428\n",
      "#SAMPLED TEXT#\n",
      "AN:\n",
      "Us themt.\n",
      "\n",
      "AUMWINMA  WHREY:\n",
      "Petshive gos.\n",
      "\n",
      "GEN:\n",
      "Whatod dipsing mume thantthis youd thoce your onisw, upesent?\n",
      "\n",
      "STELIO:\n",
      "Cat thill?\n",
      "\n",
      "FREMZR:\n",
      "UVk!\n",
      "\n",
      "TIC:\n",
      "An we whing a me. Thitt!\n",
      "\n",
      "LADDY:\n",
      "Have to dom spyte is.\n",
      "\n",
      "LUKE:\n",
      "The  you you dt an wied don't Thinkt spdic ride?\n",
      "\n",
      "JOLEOT:\n",
      "####END###\n",
      "\n",
      "Epoch 30/200, Batch 0/3947\n",
      "Current batch loss = 75.79983172\n",
      "#SAMPLED TEXT#\n",
      "IthoophkpgithoikitfonthiigothigvithanktigheifixhtHobigohkontixhainhixthowknthougozexefy\n",
      "\n",
      "HAHCY:\n",
      "E:\n",
      "DILIA:\n",
      "\n",
      "Y:\n",
      "JD:\n",
      "JY:\n",
      "JHY:\n",
      "JE:\n",
      "JK:\n",
      "FR:\n",
      "\n",
      "Y:\n",
      "PIHAH:\n",
      "JCK:\n",
      "Y:\n",
      "H:\n",
      "JY:\n",
      "JH:\n",
      "JY:\n",
      "Z:\n",
      "\n",
      "\n",
      "JUH:\n",
      "Y:\n",
      "JU:\n",
      "JY:\n",
      "JIHCHAPkg kahkthithighikiththithigothagthabpthinhighon.\n",
      "\n",
      "H:\n",
      "J:\n",
      "Y:\n",
      "JO:\n",
      "JIDD:\n",
      "FH:\n",
      "JII\n",
      "####END###\n",
      "\n",
      "Epoch 40/200, Batch 0/3947\n",
      "Current batch loss = 61.3924528959\n",
      "#SAMPLED TEXT#\n",
      "OON:\n",
      "I Seding.\n",
      "\n",
      "LOON:\n",
      "I'Dorest cocrelth dowt me they only.\n",
      "\n",
      "EVERETFR\n",
      "FREGORA:\n",
      "I'h unst blan igh take do.\n",
      "\n",
      "SIPAN:\n",
      "Non't Ture if I lorge in kileand ourremy.\n",
      "\n",
      "EDD:\n",
      "Yoaf fight itent tack Dy.\n",
      "\n",
      "PLENALLIEAR:\n",
      "Om feadelonstomonded norefattedverrisningungmaher don've to there Tirer \n",
      "####END###\n",
      "\n",
      "Epoch 50/200, Batch 0/3947\n",
      "Current batch loss = 65.746310604\n",
      "#SAMPLED TEXT#\n",
      "ONNIE:\n",
      "You tell Fuverys brot  oned spy that no, . h!!\n",
      "\n",
      "WLEIMAN:\n",
      "Fidn't someen's there, becom the my.   Che have suck quongs undhey nowed do this for Acll  shapro tess look?\n",
      "\n",
      "PACALKOVER:\n",
      "No you ge, won'n just hered till, thom!\n",
      "\n",
      "JOMY:\n",
      "Him Thele wonnich inkeada bello's  a som\n",
      "####END###\n",
      "\n",
      "Epoch 60/200, Batch 0/3947\n",
      "Current batch loss = 64.8477721596\n",
      "#SAMPLED TEXT#\n",
      "EN:\n",
      "Whaid unders om soit?\n",
      "\n",
      "LEIA:\n",
      "The reriing neoneack! Yan hua mive shot.\n",
      "\n",
      "EDDY:\n",
      "Reelf seent litt.  Is lever, tihe wai?\n",
      "\n",
      "DONNIE:\n",
      "Ky.\n",
      "\n",
      "BEDDDID:\n",
      "Nos, but lonnirighshop hure job I plod. I'r?\n",
      "\n",
      "BUDDY:\n",
      "You you'r!\n",
      "\n",
      "PANDY:\n",
      "Edam. SHered ansiright?\n",
      "\n",
      "GRETENSOR.  GODON. TRE EPEmy a ss\n",
      "####END###\n",
      "\n",
      "Epoch 70/200, Batch 0/3947\n",
      "Current batch loss = 63.7103536748\n",
      "#SAMPLED TEXT#\n",
      "R. THUKT:\n",
      "Then for on reve beave a bsswasing at or, you there make is donn stallye toly do I far. Therad you all come anyieve Did clatigetceting my.\n",
      "\n",
      "DUKE Meen with.\n",
      "\n",
      "ELAINN:\n",
      "Good unfut yoursal, Yoof  Then?\n",
      "\n",
      "GONZO:\n",
      "Recch. There about lifterseashy, we're no....5 Thing the t\n",
      "####END###\n",
      "\n",
      "Epoch 80/200, Batch 0/3947\n",
      "Current batch loss = 62.5452178464\n",
      "#SAMPLED TEXT#\n",
      "NGAN:\n",
      "Pifrsof the orge ings, extrise with feel enur he eris the Pay.  Whem is do shous befomorry had, lige you mostst's the cald aied so que ppents and tast? About morniston.\n",
      "\n",
      "HAN:\n",
      "Why tressesting unond is new it with propy the fur thing  appy nexput don't know. Wo and reb\n",
      "####END###\n",
      "\n",
      "Epoch 90/200, Batch 0/3947\n",
      "Current batch loss = 62.9092307924\n",
      "#SAMPLED TEXT#\n",
      "ONZO:\n",
      "End buert tilge, Min't was nigen lell thad bust black thinkingke dnid Alltectry, souts ame going?\n",
      "\n",
      "CHANNTER:\n",
      "If Clock.\n",
      "\n",
      "LUKE:\n",
      "..\n",
      "\n",
      "DETYDOMMMMIPY:\n",
      "Of as tahk ya that's up to of the coo you if on all anntle matting greaust nist on you'd have to os. I know en be! What al\n",
      "####END###\n",
      "\n",
      "Epoch 100/200, Batch 0/3947\n",
      "Current batch loss = 64.1972375798\n",
      "#SAMPLED TEXT#\n",
      "AN:\n",
      "I do ching in gepistely must extit's me a be a gacciend is muftitat.  Asten Siling onast.\n",
      "\n",
      "LUKE:\n",
      "No.  limabassed'mncmed a mills?\n",
      "\n",
      "RAMATHIN:\n",
      "No.\n",
      "\n",
      "EVERETT:\n",
      "The rcimy ilfbout. He's wesle of it?\n",
      "\n",
      "DONNIE:\n",
      "What symagay.\n",
      "\n",
      "RORE:\n",
      "You go anetion.  Let's dhap inngcong, He didn'th\n",
      "####END###\n",
      "\n",
      "Epoch 110/200, Batch 0/3947\n",
      "Current batch loss = 64.6786373444\n",
      "#SAMPLED TEXT#\n",
      "ENN:\n",
      "Cer. That'il you can't not nothan the ver size seve us is to whing you and mack.\n",
      "\n",
      "PANDY:\n",
      "I tourse there bacalan fir bordges? Ke dariman, you don't on not you thele on as lell spicerin's bonifton.\n",
      "\n",
      "JOHN:\n",
      "Buth.\n",
      "\n",
      "DONNIE:\n",
      "I'm sot runk this cones. Fove midds will dalak wer\n",
      "####END###\n",
      "\n",
      "Epoch 120/200, Batch 0/3947\n",
      "Current batch loss = 67.9964953187\n",
      "#SAMPLED TEXT#\n",
      "ONACK:\n",
      "Whiming bin's a dind! And you. I too.  Te, hir joikes, Sheny stre Bover therring, ootoncather in worky.\n",
      "\n",
      "DETECTIVE IMGEFE DIMMMMYONEY:\n",
      "Wampposing  you'l were nownike, betom witnt muctab?\n",
      "\n",
      "VARUCHR:\n",
      "The Looks. be uffy how thoucto! that you it did aingh comelid.\n",
      "\n",
      "THEO:\n",
      "####END###\n",
      "\n",
      "Epoch 130/200, Batch 0/3947\n",
      "Current batch loss = 66.1422778417\n",
      "#SAMPLED TEXT#\n",
      "AN:\n",
      "Cr a souse incieselling.\n",
      "\n",
      "REESPIL:\n",
      "He was about the a like no you'c stors.\n",
      "\n",
      "REESMONRVE:\n",
      "I hanting the wat Ifthou nead to can't are tone founrs himping to each at im fight ladrin' the arely, you yold with this knowe.\n",
      "\n",
      "BENTH:\n",
      "Yesoing bifse was you'vesQe, was heroth Lryou\n",
      "####END###\n",
      "\n",
      "Epoch 140/200, Batch 0/3947\n",
      "Current batch loss = 66.7763172153\n",
      "#SAMPLED TEXT#\n",
      "AYRYS ONE:\n",
      "Love. you you futcato. That's guitufacke way icke.\n",
      "\n",
      "DUKE:\n",
      "Oher, esss outastenty.  Obor Me life, necoudsicuce wanticing?..tiae.\n",
      "\n",
      "PROMIZARAN:\n",
      "No the intemoch sels of in.\n",
      "\n",
      "DELMAR:\n",
      "He's, and But thenty. \n",
      "\n",
      "BUDDY:\n",
      "He's everybody, midda perect we? The meer a ling So. I\n",
      "####END###\n",
      "\n",
      "Epoch 150/200, Batch 0/3947\n",
      "Current batch loss = 67.4607420719\n",
      "#SAMPLED TEXT#\n",
      "RETCHEN:\n",
      "What choruct in stonessard the yorg he canet....Re. FILo Th.?  Keone haply litt nergh!\n",
      "\n",
      "DE. POLAR.!! That damn.\n",
      "\n",
      "SANDY:\n",
      "Trun to to goingummach a ling a stur buttyaning.  I've life to it of hin bears ...it. Fiked, Comptionowore.\n",
      "\n",
      "PROFESPERIE:\n",
      "Whetes!\n",
      "\n",
      "SANAH:\n",
      "Pough \n",
      "####END###\n",
      "\n",
      "Epoch 160/200, Batch 0/3947\n",
      "Current batch loss = 68.4617905057\n",
      "#SAMPLED TEXT#\n",
      "ANNILIEA:\n",
      "They wenly Lilt good and Decturbe iopsrile whore we pascial throus a dogive hograpiand bah. I cartioin to Corins. I loin?\n",
      "\n",
      "DONNIE:\n",
      "Fover!\n",
      "\n",
      "DONNIE:\n",
      "Mr. tolte arone. Wo mey, sCow, rema mear?\n",
      "\n",
      "KAREND:\n",
      "Why!\n",
      "\n",
      "DWIGHE:\n",
      "I don't kids is to see, intrer to do lans. I'm abou\n",
      "####END###\n",
      "\n",
      "Epoch 170/200, Batch 0/3947\n",
      "Current batch loss = 70.6818801726\n",
      "#SAMPLED TEXT#\n",
      "OWAY:\n",
      "Ohif think a no talk!\n",
      "\n",
      "LEIA:\n",
      "Greaving up upped about aft, Rava Loa the soicnic!   Ca don't?\n",
      "\n",
      "VID:\n",
      "Whet make to this onn to no perfustionf in thether'd.\n",
      "\n",
      "GONZO:\n",
      "Lorg is farter goonep my so the fightspinf ip!\n",
      "\n",
      "DECKARD:\n",
      "Say hund sidise agay the is?\n",
      "\n",
      "KAREEA:\n",
      "Yes, was all\n",
      "####END###\n",
      "\n",
      "Epoch 180/200, Batch 0/3947\n",
      "Current batch loss = 69.4313656471\n",
      "#SAMPLED TEXT#\n",
      "ONNIE:\n",
      "A lostin On.\n",
      "\n",
      "BATTY:\n",
      "What whut heel the core one Ertyliom and figun agatestempse.\n",
      "\n",
      "DG GAR:\n",
      "That abowhre!.. I'll wanto is are nick your like he say?\n",
      "\n",
      "DENSA:\n",
      "Sninge worbck to tell I dather pations and crangent's you're. ThIf is father move, love and donncake lattle in\n",
      "####END###\n",
      "\n",
      "Epoch 190/200, Batch 0/3947\n",
      "Current batch loss = 68.5788666144\n",
      "#SAMPLED TEXT#\n",
      "ANDO:\n",
      "Walks, but's stsoction he win to call a backie. I have to do there is the poiclepidend it that to a ridew the ronires dirn't I betckere.\n",
      "\n",
      "MALWMY:\n",
      "I lefthind.\n",
      "\n",
      "BENTE:\n",
      "Peray Obare.\n",
      "\n",
      "COMMISB:\n",
      "But of have thing! Kefoam guents?\n",
      "\n",
      "COWBE, ANDAR PERE CAGyKed whery rement to s\n",
      "####END###\n",
      "\n",
      "Epoch 200/200, Batch 0/3947\n",
      "Current batch loss = 66.9181906919\n",
      "#SAMPLED TEXT#\n",
      "ANDY:\n",
      "You'd with in my you takeh Sest for frient some innitightive you was by.\n",
      "\n",
      "KARONETR:\n",
      "What a full be a too shend Stataygrillide of what you suren I'd puties. A? I ass you and at tother have was genan.\n",
      "\n",
      "ELIVAY:\n",
      "You're an gele to The Loblicin. That cortone... cad her Com\n",
      "####END###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "D = Dataset(5,30)\n",
    "D.preprocess(\"input.txt\")\n",
    "D.create_minibatches()\n",
    "run_language_model(D,200,sample_every=3500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
