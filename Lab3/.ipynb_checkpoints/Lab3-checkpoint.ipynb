{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5,  8],\n",
       "        [11, 18]],\n",
       "\n",
       "       [[11, 18],\n",
       "        [11, 18]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([[[1,2],[3,4]],[[3,4],[3,4]]]),np.array([[1,2],[2,3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset modul valjda..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]]\n",
      "[[0 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [1 2]]\n",
      "[[0 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [1 2]]\n",
      "[0 3 2 3]\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "[[1 3 2]\n",
      " [1 3 1]\n",
      " [0 1 0]]\n",
      "\n",
      "[3 3 1]\n",
      "(3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "zip(['a','b','c'], range(3))\n",
    "temp_b = np.random.randint(2,size=(1,2))\n",
    "temp = np.random.randint(3,size=(4,2))\n",
    "print temp_b\n",
    "print temp\n",
    "print temp + temp_b\n",
    "print np.sum(temp,1)\n",
    "for i in reversed(range(5)):\n",
    "    print i\n",
    "\n",
    "test = np.random.randint(4,size=(3,3))\n",
    "print test\n",
    "print\n",
    "print test[:,1]\n",
    "\n",
    "t1 = np.random.randint(4,size=(2,2))\n",
    "t2 = np.random.randint(4,size=(2,2))\n",
    "t3 = np.random.randint(4,size=(2,2))\n",
    "tem = []\n",
    "tem.append(t1)\n",
    "tem.append(t2)\n",
    "tem.append(t3)\n",
    "print np.array(tem).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "minibatches = 3\n",
    "encoded_sequence = np.array([1,1,2])\n",
    "new_sequence = np.zeros((minibatches, 3)) # encode in 1-of-k representation\n",
    "new_sequence[range(minibatches),encoded_sequence] = 1\n",
    "print new_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u't', u'e', u'\\n', u'a', u'c', u'b', u'f', u'i', u'l', u'3', u'1', u's', u'2', u'4']\n",
      "\n",
      "{u'a': 3, u'c': 4, u'b': 5, u'e': 1, u'f': 6, u'i': 7, u's': 11, u'\\n': 2, u'l': 8, u'1': 10, u'3': 9, u'2': 12, u't': 0, u'4': 13}\n",
      "\n",
      "{0: u't', 1: u'e', 2: u'\\n', 3: u'a', 4: u'c', 5: u'b', 6: u'f', 7: u'i', 8: u'l', 9: u'3', 10: u'1', 11: u's', 12: u'2', 13: u'4'}\n",
      "\n",
      "[ 0  1 11  0  6  7  8  1  2 10 12  9 13  2  3  5  4  0  0  0]\n",
      "\n",
      "[ 0  1 11  0]\n",
      "\n",
      "[u't' u't' u'e' u'e']\n",
      "\n",
      "minibatch\n",
      "### Create_minibatches print ### batch number 0\n",
      "[(array([ 0,  1, 11,  0]), array([ 1, 11,  0,  6])), (array([6, 7, 8, 1]), array([7, 8, 1, 2]))]\n",
      "### Create_minibatches print ### batch number 1\n",
      "[(array([ 2, 10, 12,  9]), array([10, 12,  9, 13])), (array([13,  2,  3,  5]), array([2, 3, 5, 4]))]\n",
      "None\n",
      "\n",
      "(False, array([[ 0,  1, 11,  0],\n",
      "       [ 6,  7,  8,  1]]), array([[ 1, 11,  0,  6],\n",
      "       [ 7,  8,  1,  2]]))\n",
      "\n",
      "(False, array([[ 2, 10, 12,  9],\n",
      "       [13,  2,  3,  5]]), array([[10, 12,  9, 13],\n",
      "       [ 2,  3,  5,  4]]))\n",
      "\n",
      "(True, array([[ 0,  1, 11,  0],\n",
      "       [ 6,  7,  8,  1]]), array([[ 1, 11,  0,  6],\n",
      "       [ 7,  8,  1,  2]]))\n",
      "\n",
      "(False, array([[ 2, 10, 12,  9],\n",
      "       [13,  2,  3,  5]]), array([[10, 12,  9, 13],\n",
      "       [ 2,  3,  5,  4]]))\n",
      "\n",
      "test (True, array([[ 0,  1, 11,  0],\n",
      "       [ 6,  7,  8,  1]]), array([[ 1, 11,  0,  6],\n",
      "       [ 7,  8,  1,  2]]))\n",
      "True\n",
      "[11  8]\n",
      "[[ 1 11  0  6]\n",
      " [ 7  8  1  2]]\n"
     ]
    }
   ],
   "source": [
    "# data = \"testfile\\n1234\\nabcttt\"\n",
    "# dictionary ={}\n",
    "# for letter in data:\n",
    "#     if letter in dictionary:\n",
    "#         dictionary[letter] += 1\n",
    "#     else:\n",
    "#         dictionary[letter] = 1\n",
    "# sorted_x = sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True)\n",
    "# print sorted_x\n",
    "# print map(lambda x: x[0],sorted_x)\n",
    "\n",
    "D = Dataset(2,4)\n",
    "D.preprocess(\"untitled.txt\")\n",
    "print D.sorted_chars\n",
    "print\n",
    "print D.char2id\n",
    "print\n",
    "print D.id2char\n",
    "print\n",
    "print D.x\n",
    "print\n",
    "print D.encode('test')\n",
    "print\n",
    "print D.decode([0,0,1,1])\n",
    "print\n",
    "print \"minibatch\"\n",
    "print D.create_minibatches()\n",
    "print\n",
    "print D.next_minibatch()\n",
    "print\n",
    "print D.next_minibatch()\n",
    "print\n",
    "print D.next_minibatch()\n",
    "print\n",
    "print D.next_minibatch()\n",
    "print\n",
    "test2 = D.next_minibatch()\n",
    "print \"test\", test2\n",
    "print test2[0]\n",
    "print test2[1][:,2]\n",
    "print test2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "    # \"np\" stands for numpy.\n",
    "    def __init__(self, batch_size, sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_pointer = 0\n",
    "        \n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read().decode(\"utf-8\")\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        \n",
    "        dictionary ={}\n",
    "        for letter in data:\n",
    "            if letter in dictionary:\n",
    "                dictionary[letter] += 1\n",
    "            else:\n",
    "                dictionary[letter] = 1\n",
    "        sorted_x = sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        self.sorted_chars = map(lambda x: x[0],sorted_x)\n",
    "        \n",
    "        # self.sorted chars contains just the characters ordered descending by frequency\n",
    "        self.char2id = dict(zip(self.sorted_chars, range(len(self.sorted_chars)))) \n",
    "        # reverse the mapping\n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        # convert the data to ids\n",
    "        self.x = np.array(map(self.char2id.get, data))\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        # returns the sequence encoded as integers\n",
    "        return np.array(map(self.char2id.get, sequence))\n",
    "\n",
    "    def decode(self, encoded_sequence):\n",
    "        # returns the sequence decoded as letters\n",
    "        return np.array(map(self.id2char.get, encoded_sequence))\n",
    "\n",
    "    # ...\n",
    "    \n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "\n",
    "    def create_minibatches(self):\n",
    "        self.num_batches = int(len(self.x) / (self.batch_size * self.sequence_length)) # calculate the number of batches\n",
    "        # Is all the data going to be present in the batches? Why?\n",
    "        # What happens if we select a batch size and sequence length larger than the length of the data?\n",
    "\n",
    "        #######################################\n",
    "        #       Convert data to batches       #\n",
    "        #######################################\n",
    "        if len(self.x) < self.sequence_length+1:\n",
    "            raise Exception(\"Nedovoljno podataka za jednu kombinaciju ulaza/izlaza sa zadanim sequence lengthom\")\n",
    "            \n",
    "        if self.num_batches == 0:\n",
    "            raise Exception(\"Nemoguce napraviti niti jedan batch sa zadanim parametrima\")\n",
    "        self.batches = []\n",
    "        cnt = 0\n",
    "        for j in range(self.num_batches):\n",
    "            batch = []\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            for i in range(self.batch_size):\n",
    "                inputs = np.array([elem for elem in self.x[(cnt)*self.sequence_length:(cnt+1)*self.sequence_length]])\n",
    "                targets = np.array([elem for elem in self.x[(cnt)*self.sequence_length+1:(cnt+1)*self.sequence_length+1]])\n",
    "                batch_x.append(inputs)\n",
    "                batch_y.append(targets)\n",
    "                batch.append((inputs,targets))\n",
    "                cnt +=1\n",
    "            self.batches.append((np.array(batch_x), np.array(batch_y)))\n",
    "            print \"### Create_minibatches print ### batch number %s\" % j\n",
    "            print batch\n",
    "            \n",
    "    \n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "    def next_minibatch(self):\n",
    "        # ...\n",
    "        new_epoch = False\n",
    "        if self.batch_pointer >= self.num_batches:\n",
    "            self.batch_pointer = 0\n",
    "            new_epoch = True\n",
    "            \n",
    "        batch_x, batch_y = self.batches[self.batch_pointer]\n",
    "        self.batch_pointer += 1\n",
    "        # handling batch pointer & reset\n",
    "        # new_epoch is a boolean indicating if the batch pointer was reset\n",
    "        # in this function call\n",
    "        return new_epoch, batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN dio..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "    # \"np\" stands for numpy\n",
    "    def __init__(self, hidden_size, sequence_length, vocab_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.U = np.random.randn(vocab_size, hidden_size)*0.01 # ... input projection\n",
    "        self.W = np.random.randn(hidden_size, hidden_size)*0.01 # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros((1, hidden_size)) # ... input bias\n",
    "\n",
    "        self.V = np.random.randn(hidden_size,vocab_size)*0.01 # ... output projection\n",
    "        self.c = np.zeros((1, vocab_size)) # ... output bias\n",
    "\n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U, self.memory_W, self.memory_V = np.zeros_like(U), np.zeros_like(W), np.zeros_like(V)\n",
    "        self.memory_b, self.memory_c = np.zeros_like(b), np.zeros_like(c)\n",
    "        \n",
    "        \n",
    "    def encode_1_of_k_rep(self, encoded_sequence): #encodes array of integers for a batch to 1-of-k representation\n",
    "        minibatches = encoded_sequence.shape() \n",
    "        new_sequence = np.zeros((minibatches, self.vocab_size)) # encode in 1-of-k representation\n",
    "        new_sequence[range(minibatches),encoded_sequence] = 1\n",
    "        return new_sequence\n",
    "        \n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "\n",
    "        cache = {}\n",
    "        hidden_state = np.tanh(np.dot(x, self.U) + np.dot(h_prev,self.W) + self.b) #minibatch x hidden\n",
    "#         output = np.dot(hidden_state, self.V) + self.c #minibatch x vocab_size\n",
    "#         softmax_output = np.exp(output)/np.sum(np.exp(output),1)\n",
    "        \n",
    "        cache[\"input\"] = x\n",
    "        cache[\"hprev\"] = h_prev\n",
    "        cache[\"hstate\"] = hidden_state\n",
    "#         cache[\"softmax_out\"] = softmax_output\n",
    "        \n",
    "        return hidden_state.copy(), cache\n",
    "\n",
    "\n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "\n",
    "        \n",
    "        #x je meni shape minibatch size x sequence_length x 1 gdje je 1 zapravo integer reprezentacija charactera\n",
    "        #koju encodiram prije predaje funkciji\n",
    "        h, cache = {}, {}\n",
    "        h[-1] = h0\n",
    "        hprev = h0\n",
    "        minibatches, seq_length = x.shape()\n",
    "        for i in range(seq_length):\n",
    "            current_timestep_inputs = x[range(minibatches),i]\n",
    "            current_timestep_inputs_encoded = self.encode_1_of_k_rep(current_timestep_inputs)\n",
    "            new_hidden, new_cache = self.rnn_step_forward(current_timestep_inputs_encoded,hprev,None,None,None)\n",
    "            hprev = new_hidden\n",
    "            cache[i] = new_cache\n",
    "            h[i]= new_hidden.copy()\n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "\n",
    "        return h, cache\n",
    "    \n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "\n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass\n",
    "        current_input = cache[\"input\"]\n",
    "        h_prev = cache[\"hprev\"]\n",
    "        hidden_state = cache[\"hstate\"]\n",
    "        \n",
    "        dh_prev, dU, dW, db = np.zeros_like(h_prev), np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.b)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # HINT: you can use the chain rule to compute the derivative of the\n",
    "        # hyperbolic tangent function and use it to compute the gradient\n",
    "        # with respect to the remaining parameters\n",
    "        \n",
    "        ## backprop into h\n",
    "        ## derivative of error with regard to the output of hidden layer\n",
    "        ## derivative of H, come from output layer y and also come from H(t+1), the next time H\n",
    "        #dh = np.dot(Why.T, dy) + dhnext #ovo je zapravo grad next dimenzija batch_size x hidden x hidden\n",
    "        \n",
    "        dhraw = np.dot(grad_next,(1 - hidden_state * hidden_state).transpose()) # grad_next * hidden_size x minibatch size\n",
    "        db += dhraw\n",
    "\n",
    "        ## derivative of the error with regard to the weight between input layer and hidden layer\n",
    "        dU += np.dot(dhraw, xs[t].T)\n",
    "        dW += np.dot(dhraw, hs[t-1].T)\n",
    "        ## derivative of the error with regard to H(t+1)\n",
    "        ## or derivative of the error of H(t-1) with regard to H(t)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "        return dprev_h, dWx, dWh, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        dU, dW, db = None, None, None\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        # Why are we not computing the gradient with respect to inputs (x)?\n",
    "\n",
    "        return dU, dW, db\n",
    "    \n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "\n",
    "    def output(h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        output = np.dot(h, self.V) + self.c #minibatch x vocab_size\n",
    "        softmax_output = np.exp(output)/np.sum(np.exp(output),1)\n",
    "        return softmax_output\n",
    "\n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a one-hot vector of dimension \n",
    "        #     vocabulary size x 1 - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a dictionary.\n",
    "\n",
    "        loss, dh, dV, dc = None, None, None, None\n",
    "        # calculate the output (o) - unnormalized log probabilities of classes\n",
    "        # calculate yhat - softmax of the output\n",
    "        # calculate the cross-entropy loss\n",
    "        # calculate the derivative of the cross-entropy softmax loss with respect to the output (o)\n",
    "        # calculate the gradients with respect to the output parameters V and c\n",
    "        # calculate the gradients with respect to the hidden layer h\n",
    "        softmax_output = self.output(h,V,c)\n",
    "        \n",
    "\n",
    "        return loss, dh, dV, dc\n",
    "    \n",
    "    # ...\n",
    "    # Code is nested in class definition, indentation is not representative.\n",
    "\n",
    "    # The inputs to the function are just indicative since the variables are mostly present as class properties\n",
    "\n",
    "    def update(self, dU, dW, db, dV, dc,\n",
    "                     U, W, b, V, c,\n",
    "                     memory_U, memory_W, memory_b, memory_V, memory_c):\n",
    "\n",
    "        # update memory matrices\n",
    "        # perform the Adagrad update of parameters\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "# code not necessarily nested in class definition\n",
    "\n",
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "    \n",
    "    vocab_size = len(dataset.sorted_chars)\n",
    "    RNN = None # initialize the recurrent network\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((hidden_size, 1))\n",
    "\n",
    "    average_loss = 0\n",
    "\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((hidden_size, 1))\n",
    "            # why do we reset the hidden state here?\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh, y_oh = None, None\n",
    "\n",
    "        # Run the recurrent network on the current batch\n",
    "        # Since we are using windows of a short length of characters,\n",
    "        # the step function should return the hidden state at the end\n",
    "        # of the unroll. You should then use that hidden state as the\n",
    "        # input for the next minibatch. In this way, we artificially\n",
    "        # preserve context between batches.\n",
    "        loss, h0 = RNN.step(h0, x_oh, y_oh)\n",
    "\n",
    "        if batch % sample_every == 0: \n",
    "            # run sampling (2.2)\n",
    "            pass\n",
    "        batch += 1\n",
    "        \n",
    "# ...\n",
    "# code not necessarily nested in class definition\n",
    "def sample(seed, n_sample):\n",
    "    h0, seed_onehot, sample = None, None, None \n",
    "    # inicijalizirati h0 na vektor nula\n",
    "    # seed string pretvoriti u one-hot reprezentaciju ulaza\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
